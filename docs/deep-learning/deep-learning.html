<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Deep Learning · datadocs</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="- Deep learning is a branch of machine learning that covers the set of algorithms that model complex patterns by feeding data through multiple non-linear transformations causing each level to capture a different level of abstraction."/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Deep Learning · datadocs"/><meta property="og:type" content="website"/><meta property="og:url" content="https://polakowo.github.io/datadocs/"/><meta property="og:description" content="- Deep learning is a branch of machine learning that covers the set of algorithms that model complex patterns by feeding data through multiple non-linear transformations causing each level to capture a different level of abstraction."/><meta property="og:image" content="https://polakowo.github.io/datadocs/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://polakowo.github.io/datadocs/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/datadocs/img/favicon.ico"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-142521178-1"></script><script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments); }
              gtag('js', new Date());
              gtag('config', 'UA-142521178-1');
            </script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/datadocs/js/code-block-buttons.js"></script><script type="text/javascript" src="/datadocs/js/disqus.js"></script><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/datadocs/js/scrollSpy.js"></script><link rel="stylesheet" href="/datadocs/css/prism.css"/><link rel="stylesheet" href="/datadocs/css/main.css"/><script src="/datadocs/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/datadocs/"><h2 class="headerTitle">datadocs</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/datadocs/docs/machine-learning/linear-models" target="_self">Docs</a></li><li class=""><a href="https://github.com/polakowo/datadocs" target="_self">GitHub</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>General</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle collapsible">Machine Learning<span class="arrow"><svg width="24" height="24" viewBox="0 0 24 24"><path fill="#565656" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path d="M0 0h24v24H0z" fill="none"></path></svg></span></h3><ul class="hide"><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">General</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/machine-learning">Machine Learning</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Methods</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/linear-models">Linear Models</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/tree-based-models">Tree-Based Models</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/ensemble-methods">Ensemble Methods</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Features</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/eda">Exploratory Data Analysis</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/feature-engineering">Feature Engineering</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/advanced-features">Advanced Features</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Optimization</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/metric-optimization">Metric Optimization</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/validation-schemes">Validation Schemes</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/hyperopt">Hyperparameter Optimization</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Competitions</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/competitive-ml">Competitive Machine Learning</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/data-leakages">Data Leakages</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Production</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/production-code">Production Code</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/deployment">Deployment</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/deployment-to-cloud">Deployment to Cloud</a></li></ul></div></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle collapsible">Deep Learning<span class="arrow"><svg width="24" height="24" viewBox="0 0 24 24"><path fill="#565656" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path d="M0 0h24v24H0z" fill="none"></path></svg></span></h3><ul class="hide"><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">General</h4><ul><li class="navListItem navListItemActive"><a class="navItem" href="/datadocs/docs/deep-learning/deep-learning">Deep Learning</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/dl-strategy">Deep Learning Strategy</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Fundamentals</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/backpropagation">Backpropagation</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/activation-functions">Activation Functions</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/initialization">Initialization</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/optimization">Optimization</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/regularization">Regularization</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Computer Vision</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/cnns">Convolutional Neural Networks</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/cnn-architectures">CNN Architectures</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/object-detection">Object Detection</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/face-recognition">Face Recognition</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/nst">Neural Style Transfer</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Natural Language Processing</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/rnns">Recurrect Neural Networks</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/word-embeddings">Word Embeddings</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/nmt">Neural Machine Translation</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/attention-mechanisms">Attention Mechanisms</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/speech-recognition">Speech Recognition</a></li></ul></div></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle collapsible">Big Data<span class="arrow"><svg width="24" height="24" viewBox="0 0 24 24"><path fill="#565656" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path d="M0 0h24v24H0z" fill="none"></path></svg></span></h3><ul class="hide"><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">General</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/big-data">Big Data</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/data-warehousing">Data Warehousing</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/data-lakes">Data Lakes</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/data-pipelines">Data Pipelines</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Databases</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/database-design">Database Design</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/sql-databases">SQL Databases</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/wide-column-stores">Wide Column Stores</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/key-value-stores">Key Value Stores</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/document-stores">Document Stores</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/graph-stores">Graph Stores</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Hadoop</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/hadoop">Hadoop Ecosystem</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/data-ingestion">Data Ingestion</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/data-storage">Data Storage</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/data-processing">Data Processing</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/query-engines">Query Engines</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/cluster-management">Cluster Management</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Cloud</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/cloud-computing">Cloud Computing</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/aws">Amazon Web Services</a></li></ul></div></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/polakowo/datadocs/edit/master/docs/deep-learning/deep-learning.md" target="_blank" rel="noreferrer noopener">Edit</a><h1 class="postHeaderTitle">Deep Learning</h1></header><article><div><span><ul>
<li>Deep learning is a branch of machine learning that covers the set of algorithms that model complex patterns by feeding data through multiple non-linear transformations causing each level to capture a different level of abstraction.</li>
<li>Deep learning algorithms seek to exploit the unknown structure in the input distribution.</li>
<li>The word “learning” describes an automatic search process for better representation of the data.
<ul>
<li>The key aspect of deep learning is that the layers of features are not designed by human engineers.</li>
<li>A deep learning process can learn which features to optimally place in which level on its own.</li>
</ul></li>
<li>Most modern deep learning models are based on an ANNs.</li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="compared-to-ai-and-ml"></a><a href="#compared-to-ai-and-ml" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Compared to AI and ML</h4>
<ul>
<li>AI &gt;&gt; ML &gt;&gt; DL</li>
<li>AI enables machines to mimic human intelligence, using logic and machine learning.</li>
<li>ML is a subset of AI that enables machines to improve at tasks with experience.</li>
<li>DL is a subset of ML that exposes multi-layered neural networks to vast amounts of data.</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="rise-of-deep-learning"></a><a href="#rise-of-deep-learning" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Rise of deep learning</h2>
<ul>
<li>Deep learning is taking off due to a large amount of data available through:
<ul>
<li>Digitization of the society</li>
<li>Faster computation</li>
<li>Innovation in the development of neural network algorithms</li>
</ul></li>
</ul>
<blockquote>
<p>Previously, the labeled datasets were too small, the computers were too slow, the weights were initialized poorly, and the wrong type of non-linearity was used - Geoff Hinton</p>
</blockquote>
<ul>
<li>Neural networks are extremely better than traditional methods because of the advances in hardware (mostly GPUs, now TPUs), and the exponential generation and accumulation of data.</li>
</ul>
<p><img width=450 src="/datadocs/assets/riseofDeeplearning.jpg"/>
<center><a href="http://bytes.schibsted.com/deep-learning-changing-data-science-paradigms/" class="credit">Credit</a></center></p>
<ul>
<li>Another reason that deep learning has taken off is because of important theoretical and algorithmic improvements, mainly in supervised learning.</li>
</ul>
<p><img width=900 src="/datadocs/assets/deepLearningTimeline.png"/>
<center><a href="https://towardsdatascience.com/a-weird-introduction-to-deep-learning-7828803693b0" class="credit">Credit</a></center></p>
<h2><a class="anchor" aria-hidden="true" id="hierarchical-learning"></a><a href="#hierarchical-learning" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hierarchical learning</h2>
<ul>
<li>Deep learning is a class of ML algorithms that learn a hierarchical representation of the data.</li>
<li>Each level learns to transform its input data into a slightly more abstract and composite representation.</li>
<li>The most attractive thing about DL is that it is motivated by the way the world works:
<ul>
<li>The world is compositional, atoms form molecules, and molecules form organisms.</li>
</ul></li>
<li>In literature we have <em>letters-&gt;words-&gt;sentences-&gt;paragraph</em>, just as in vision we have <em>pixels-&gt;edges-&gt;parts-&gt;objects</em>.</li>
</ul>
<p><img width=500 src="/datadocs/assets/featureLearning.jpg"/>
<center><a href="https://www.picswe.com/pics/feature-learning-a0.html" class="credit">Credit</a></center></p>
<h2><a class="anchor" aria-hidden="true" id="deep-neural-networks"></a><a href="#deep-neural-networks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Deep neural networks</h2>
<ul>
<li>Neural networks have individual units (neurons) are connected in an directed acyclic graph (DAG).</li>
<li>Deep NNs have more layers between input and output:
<ul>
<li>This allows richer intermediate representations to be built.</li>
<li>More of the feature engineering can be achieved by the algorithm itself.</li>
</ul></li>
<li>They generate compositional models where object is expressed as a layered composition of primitives.</li>
</ul>
<p><img width=400 src="/datadocs/assets/0*0mia7BQKjUAuXeqZ.jpeg"/>
<center><a href="http://cs231n.github.io/neural-networks-1/" class="credit">Credit</a></center></p>
<h4><a class="anchor" aria-hidden="true" id="universal-function-approximators"></a><a href="#universal-function-approximators" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Universal function approximators</h4>
<ul>
<li>NNs with at least one hidden layer are universal approximators:
<ul>
<li>They can compute and learn any function at all.</li>
</ul></li>
<li>Almost any process we can think of can be represented as a functional computation in NNs.</li>
</ul>
<p><img width=400 src="/datadocs/assets/decisionBoundary.jpg"/>
<center><a href="https://www.learnopencv.com/understanding-feedforward-neural-networks/" class="credit">Credit</a></center></p>
<ul>
<li>The fact that deeper networks (with multiple hidden layers) can work better than a single-hidden-layer networks is an empirical observation, despite the fact that their representational power is equal.</li>
<li><a href="http://neuralnetworksanddeeplearning.com/chap4.html">A visual proof that neural nets can compute any function</a></li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="pros"></a><a href="#pros" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Pros</h4>
<ul>
<li>Scalability of neural networks:
<ul>
<li>Scales well to larger datasets with new GPU hardware and CUDA software.</li>
<li>Results (usually) get better with more data and larger models.</li>
</ul></li>
<li>Ability to perform automatic feature extraction from raw data, also called feature learning.</li>
<li>Relatively simple learning algorithm (SGD and backpropagation)</li>
<li>Information such as in traditional programming is stored on the entire network, not on a database.</li>
<li>The disappearance of a few pieces of information in one place does not prevent the network from functioning (think of brain damage)</li>
<li>Parallel processing capabilities.</li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="cons"></a><a href="#cons" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Cons</h4>
<ul>
<li>DNNs are prone to overfitting because of the added layers of abstraction.
<ul>
<li>This allows them to learn noise.</li>
</ul></li>
<li>Hard to interpret the model.
<ul>
<li>They are a black boxes once they are trained.</li>
</ul></li>
<li>Don't perform as well on small data sets.
<ul>
<li>The Bayesian approaches do have an advantage here.</li>
</ul></li>
<li>There is no specific rule for determining appropriate structure but through trial and error.</li>
<li>They are computationally intensive to train:
<ul>
<li>You need a lot of chips and a distributed run-time to train on very large datasets.</li>
</ul></li>
<li>Sometimes DL can be an overkill:
<ul>
<li>Traditional ML algorithms still work for smaller problems with little data availability.</li>
<li>Clustering still works for some unsupervised learning problems.</li>
</ul></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="biology"></a><a href="#biology" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Biology</h2>
<ul>
<li>Deep learning models are vaguely inspired by information processing and communication patterns in biological nervous systems.</li>
<li>Yet they have various differences from the structural and functional properties of biological brains (especially human brains), which make them incompatible with neuroscience evidences.</li>
<li>For example, there are many different types of neurons, each with different properties.
<ul>
<li>The dendrites in biological neurons perform complex nonlinear computations.</li>
<li>The synapses are not just a single weight, they’re a complex non-linear dynamical system.</li>
<li><a href="https://neurophysics.ucsd.edu/courses/physics_171/annurev.neuro.28.061604.135703.pdf">Dendritic Computation</a></li>
</ul></li>
<li>Nevertheless, the computations performed by deep learning units could be similar to those of actual neurons and neural populations.
<ul>
<li>Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.</li>
</ul></li>
<li>Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.</li>
</ul>
<p><img width=500 src="/datadocs/assets/biologicalNeuron.png"/>
<center><a href="https://www.mentalconstruction.com/mental-construction/neural-connections/neural-threshold/attachment/generic-neuron-input-output/" class="credit">Credit</a></center></p>
<h2><a class="anchor" aria-hidden="true" id="architectures"></a><a href="#architectures" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Architectures</h2>
<ul>
<li>Modern state-of-the-art deep learning is focused on training deep (multi-layered) neural network models using the backpropagation algorithm.</li>
<li>Most deep learning techniques are extensions or adaptions of ANNs.</li>
<li>Different configurations are suitable for different machine learning tasks.</li>
<li><a href="https://medium.com/@Francesco_AI/ai-knowledge-map-how-to-classify-ai-technologies-6c073b969020?fbclid=IwAR1mrs0KqMNST6AwqBFFZFWJmWNs34NFoADNk_LT-3o27w2nEyFfmBB9T_Q">AI Knowledge Map: how to classify AI technologies</a></li>
</ul>
<p><img width=800 src="/datadocs/assets/knowledgeMap.jpeg"/>
<center><a href="https://www.kdnuggets.com/2018/08/ai-knowledge-map-classify-ai-technologies.html" class="credit">Credit</a></center></p>
<h2><a class="anchor" aria-hidden="true" id="challenges"></a><a href="#challenges" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Challenges</h2>
<ul>
<li>DL is a tool for perceptual classification, when general intelligence involves so much more.
<ul>
<li>The biggest obstacle in terms of real A(G)I for the current wave of Deep Learning is almost total lack of abstract reasoning.</li>
</ul></li>
<li>Back-propagation has trouble generalizing outside a space of training examples.
<ul>
<li>Therefore, current models cannot account for those cognitive phenomena that involve universals that can be freely extended to arbitrary cases.</li>
</ul></li>
<li>State-of-the-art DNNs perform image classification well but are still far from true object recognition.
<ul>
<li><a href="https://arxiv.org/pdf/1811.11553.pdf">Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects (2018)</a></li>
</ul></li>
</ul>
<p><img width=400 src="/datadocs/assets/confusion.jpeg"/>
<center><a href="https://medium.com/@GaryMarcus/the-deepest-problem-with-deep-learning-91c5991f5695" class="credit">Credit</a></center></p>
<ul>
<li>RL: We are beginning to realize that there are major complexity problems with regards to the entire ML paradigm of specifying reward functions and optimizing based on these rewards.</li>
<li>Deep learning is:
<ul>
<li>Data hungry: Lacks a mechanism for learning abstractions through explicit, verbal definition, and works best when there are millions of training examples.</li>
<li>Swallow: There is, so far, no good way to carry training from one set of circumstances to another.</li>
<li>Not sufficiently transparent: The transparency issue, as yet unsolved, is a potential liability when using deep learning for problem domains like financial trades or medical diagnosis.</li>
<li>Difficult to engineer: Engineering risks could be particularly problematic with deep learning given their statistical nature, opacity, and difficulty distinguishing causation from correlation.</li>
</ul></li>
<li><a href="https://arxiv.org/pdf/1801.00631.pdf">Deep Learning: A Critical Appraisal (2017)</a></li>
<li><a href="https://medium.com/@GaryMarcus/the-deepest-problem-with-deep-learning-91c5991f5695">The deepest problem with deep learning</a></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="future-research"></a><a href="#future-research" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Future research</h2>
<ul>
<li>Deep Learning is an active field of research:
<ul>
<li>We are still searching for the best models, topology of the networks, best ways to optimize their hyperparameters and more.</li>
</ul></li>
<li>We should and will see more benefits coming from the unsupervised side of the tracks as the field matures to deal with the abundance of unlabeled data available.
<ul>
<li>Unsupervised feature learning approaches, like Autoencoders, would automatically make conclusions from similar observations.</li>
<li>Then manually labeling these conclusions can be practical, and this is the way curiosity of computers are satisfied.</li>
</ul></li>
<li>Deep learning and symbol-manipulation will co-exist:
<ul>
<li>With deep learning handling many aspects of perceptual classification, but symbol-manipulation playing a vital role in reasoning about abstract knowledge.</li>
<li>Gradient descent plus symbols, not gradient descent alone.</li>
</ul></li>
<li>Deep Reinforcement Learning is another future direction:
<ul>
<li>The model behaves more like to a human by interacting with the noisy environment and making precise decisions upon given scalar reward value.</li>
</ul></li>
<li>Interpretation techniques for deep learning</li>
<li>Bayesian deep learning</li>
<li>Meta-learning will be the new SGD.</li>
<li>Generative models will drive a new kind of modeling.</li>
</ul>
</span></div></article></div><div class="docLastUpdate"><em>Last updated on 2019-10-15 by Oleg Polakow</em></div><div class="docs-prevnext"><a class="docs-prev button" href="/datadocs/docs/machine-learning/deployment-to-cloud"><span class="arrow-prev">← </span><span>Deployment to Cloud</span></a><a class="docs-next button" href="/datadocs/docs/deep-learning/dl-strategy"><span>Deep Learning Strategy</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#rise-of-deep-learning">Rise of deep learning</a></li><li><a href="#hierarchical-learning">Hierarchical learning</a></li><li><a href="#deep-neural-networks">Deep neural networks</a></li><li><a href="#biology">Biology</a></li><li><a href="#architectures">Architectures</a></li><li><a href="#challenges">Challenges</a></li><li><a href="#future-research">Future research</a></li></ul></nav></div><footer class="nav-footer" id="footer"><div class="brand-box"><div class="brand"><a href="https://www.tum.de/nc/en/" target="_blank" rel="noreferrer noopener" class="brand-link"><img src="/datadocs/img/tum_logo.png" alt="Technical University of Munich" height="45"/></a></div><div class="brand"><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="brand-link"><img src="/datadocs/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a></div></div><section class="copyright">Copyright © 2019 polakowo.io</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '6642fca03d716a543ac4428d7d20b842',
                indexName: 'polakowo-datadocs',
                inputSelector: '#search_input_react',
                algoliaOptions: {}
              });
            </script></body></html>