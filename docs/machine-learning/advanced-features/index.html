<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Advanced Features · datadocs</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="## Group-based features"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Advanced Features · datadocs"/><meta property="og:type" content="website"/><meta property="og:url" content="https://polakowo.github.io/datadocs/"/><meta property="og:description" content="## Group-based features"/><meta property="og:image" content="https://polakowo.github.io/datadocs/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://polakowo.github.io/datadocs/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/datadocs/img/favicon.ico"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-142521178-1"></script><script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments); }
              gtag('js', new Date());
              gtag('config', 'UA-142521178-1');
            </script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/datadocs/js/code-block-buttons.js"></script><script type="text/javascript" src="/datadocs/js/disqus.js"></script><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/datadocs/js/scrollSpy.js"></script><link rel="stylesheet" href="/datadocs/css/prism.css"/><link rel="stylesheet" href="/datadocs/css/main.css"/><script src="/datadocs/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/datadocs/"><h2 class="headerTitle">datadocs</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/datadocs/docs/machine-learning/linear-models" target="_self">Docs</a></li><li class=""><a href="https://github.com/polakowo/datadocs" target="_self">GitHub</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Features</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle collapsible">Machine Learning<span class="arrow"><svg width="24" height="24" viewBox="0 0 24 24"><path fill="#565656" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path d="M0 0h24v24H0z" fill="none"></path></svg></span></h3><ul class="hide"><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">General</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/machine-learning">Machine Learning</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Methods</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/linear-models">Linear Models</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/tree-based-models">Tree-Based Models</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/ensemble-methods">Ensemble Methods</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Features</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/eda">Exploratory Data Analysis</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/feature-engineering">Feature Engineering</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/datadocs/docs/machine-learning/advanced-features">Advanced Features</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Optimization</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/metric-optimization">Metric Optimization</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/validation-schemes">Validation Schemes</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/hyperopt">Hyperparameter Optimization</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Competitions</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/competitive-ml">Competitive Machine Learning</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/data-leakages">Data Leakages</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Production</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/production-code">Production Code</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/deployment">Deployment</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/machine-learning/deployment-to-cloud">Deployment to Cloud</a></li></ul></div></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle collapsible">Deep Learning<span class="arrow"><svg width="24" height="24" viewBox="0 0 24 24"><path fill="#565656" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path d="M0 0h24v24H0z" fill="none"></path></svg></span></h3><ul class="hide"><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">General</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/deep-learning">Deep Learning</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/dl-strategy">Deep Learning Strategy</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Fundamentals</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/backpropagation">Backpropagation</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/activation-functions">Activation Functions</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/initialization">Initialization</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/optimization">Optimization</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/regularization">Regularization</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Computer Vision</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/cnns">Convolutional Neural Networks</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/cnn-architectures">CNN Architectures</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/object-detection">Object Detection</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/face-recognition">Face Recognition</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/nst">Neural Style Transfer</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Natural Language Processing</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/rnns">Recurrect Neural Networks</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/word-embeddings">Word Embeddings</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/nmt">Neural Machine Translation</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/attention-mechanisms">Attention Mechanisms</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/deep-learning/speech-recognition">Speech Recognition</a></li></ul></div></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle collapsible">Big Data<span class="arrow"><svg width="24" height="24" viewBox="0 0 24 24"><path fill="#565656" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path d="M0 0h24v24H0z" fill="none"></path></svg></span></h3><ul class="hide"><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">General</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/big-data">Big Data</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/data-warehouses">Data Warehouses</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/data-lakes">Data Lakes</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/data-pipelines">Data Pipelines</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Databases</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/database-design">Database Design</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/sql-databases">SQL Databases</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/wide-column-stores">Wide Column Stores</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/key-value-stores">Key Value Stores</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/document-stores">Document Stores</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/graph-stores">Graph Stores</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Hadoop</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/hadoop">Hadoop Ecosystem</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/data-ingestion">Data Ingestion</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/data-storage">Data Storage</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/data-processing">Data Processing</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/query-engines">Query Engines</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/cluster-management">Cluster Management</a></li></ul></div><div class="navGroup subNavGroup"><h4 class="navGroupSubcategoryTitle">Cloud</h4><ul><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/cloud-computing">Cloud Computing</a></li><li class="navListItem"><a class="navItem" href="/datadocs/docs/big-data/aws">Amazon Web Services</a></li></ul></div></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/polakowo/datadocs/edit/master/docs/machine-learning/advanced-features.md" target="_blank" rel="noreferrer noopener">Edit</a><h1 class="postHeaderTitle">Advanced Features</h1></header><article><div><span><h2><a class="anchor" aria-hidden="true" id="group-based-features"></a><a href="#group-based-features" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Group-based features</h2>
<ul>
<li>Some datasets such as transactional data have multiple rows for an instance.</li>
<li>Treat data points as dependent on each other.</li>
<li>Group by a categorical feature and calculate various statistics such as sum and mean (e.g., mean encoding).</li>
<li>To group by a numeric feature apply binning. Binning can be applied on both categorical and numerical data to make the model more robust and prevent overfitting, however, it has a cost to the performance.</li>
<li>For aggregating categorical features either:
<ul>
<li>Select the label with the highest frequency</li>
<li>Make a pivot table</li>
<li>Apply <code>groupby</code> operation after one-hot-transforming the feature.</li>
</ul></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="distance-based-features"></a><a href="#distance-based-features" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Distance-based features</h2>
<ul>
<li>Perform a <code>groupby</code> operation on instance neighborhoods (not only geodesic).</li>
<li>More flexible but harder to implement:
<ul>
<li>(optional) Mean encode all variables to create a homogeneous feature space.</li>
<li>Calculate \(N\) nearest neighbors with some distance metric (e.g., Bray-Curtis).</li>
<li>Calculate various statistics based on the nearest \(K\) neighbors.</li>
<li>Examples:
<ul>
<li>Mean target of nearest 5, 10, 15, 500, 2000 neighbors</li>
<li>Mean distance to 10 closest neighbors (with target 0/1)</li>
</ul></li>
</ul></li>
<li>For pairs of text features calculate:
<ul>
<li>The number of matching words</li>
<li>Cosine distance between their TF-IDF vectors</li>
<li>Distance between their average word2vec vectors</li>
<li>Levenshtein distance</li>
</ul></li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="k-nn"></a><a href="#k-nn" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>k-NN</h3>
<ul>
<li>K-nearest neighbors is a non-parametric method used for classification and regression.
<ul>
<li>Although k-NN belongs to the family of ML algorithms, it does not learn anything.</li>
</ul></li>
<li>k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification.</li>
<li>The algorithm searches for \(k\)-nearest training samples using a distance metric.
<ul>
<li>Regression: takes the average of the values.</li>
<li>Classification: classified by a plurality vote.</li>
<li>Larger values of \(k\) reduce noise but also make class boundaries less distinct.</li>
</ul></li>
</ul>
<p><center><img width=200 src="/datadocs/assets/knn3.png"/></center>
<center><a href="https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/" class="credit">Credit</a></center></p>
<h4><a class="anchor" aria-hidden="true" id="pros"></a><a href="#pros" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Pros</h4>
<ul>
<li>Very simple to understand and equally easy to implement.</li>
<li>A non-parametric algorithm which means there are no assumptions to be met.</li>
<li>Given that it is a memory-based approach, it immediately adapts as new training data is collected.
<ul>
<li>Allows the algorithm to respond quickly to changes in the input during real-time use.</li>
</ul></li>
<li>Works for multi-class problems without any extra efforts.</li>
<li>Can be used both for classification and regression problems.</li>
<li>Gives the user flexibility to choose the distance metric.</li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="cons"></a><a href="#cons" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Cons</h4>
<ul>
<li>Its speed declines with the increase in the size of the dataset.</li>
<li>Suffers under curse of dimensionality: works only well for a small number of input variables.
<ul>
<li>For high-dimensional data, dimension reduction is usually performed.</li>
</ul></li>
<li>Requires features to be scaled for certain distance criteria (e.g. Euclidean, Manhattan)</li>
<li>Doesn’t perform well on imbalanced data: a more frequent class tends to dominate the prediction.</li>
<li>The accuracy can be severely degraded by the presence of noisy or irrelevant features.
<ul>
<li>Select and scale features to improve classification.</li>
</ul></li>
<li>Has no capability of dealing with missing values.</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="k-means"></a><a href="#k-means" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>K-means</h3>
<ul>
<li>A very simple algorithm used in market segmentation, computer vision, and astronomy.</li>
<li>Performs division of objects into clusters that are “similar” inside and “dissimilar” outside.</li>
<li>k-means clustering aims to partition \(n\) observations into \(k\) clusters:
<ul>
<li>Select the number of clusters \(k\) that you think is the optimal number.</li>
<li>Initialize \(k\) points as “centroids” randomly within the data space.</li>
<li>Attribute each observation to its closest centroid.</li>
<li>Update the centroids to the center of all the attributed set of observations.</li>
<li>Repeat the previous steps a fixed number of times or until all of the centroids are stable.</li>
</ul></li>
<li>One method for choosing a &quot;good&quot; \(k\) is Elbow method:
<ul>
<li>The centroid distance - the average distance between data points and cluster centroids - typically reaches some &quot;elbow&quot;; it stops decreasing at a sharp rate.</li>
</ul></li>
</ul>
<p><center><img width=300 src="/datadocs/assets/the-optimal-number-of-clusters.png"/></center>
<center><a href="https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/" class="credit">Credit</a></center></p>
<ul>
<li>Inherently, K-means is NP-hard:
<ul>
<li>K-Means finds a solution in \(O(n^{dk+1})\) in time.</li>
<li>There are some heuristics to deal with this, such as <a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html">MiniBatch K-means</a></li>
</ul></li>
<li>Other limitations:
<ul>
<li>The parameter \(k\) is known to be hard to choose.</li>
<li>Cannot be used with arbitrary distance functions or on non-numerical data.</li>
</ul></li>
<li><a href="https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-7-unsupervised-learning-pca-and-clustering-db7879568417">Unsupervised Learning: PCA and Clustering</a></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="bumper-features"></a><a href="#bumper-features" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Bumper features</h2>
<ul>
<li>For a multi-classification problem, create one-versus-all tasks and use them as features.</li>
<li><em>Is it true that the target class number is greater than 1, 2, etc.?</em></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="feature-interactions"></a><a href="#feature-interactions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Feature interactions</h2>
<ul>
<li>Construct combinations of features in order to incorporate the knowledge into a model.</li>
<li>For categorical features:
<ul>
<li>Concatenate them by using a delimiter.</li>
<li>Or vectorize into a real-valued representation and then apply the operation.</li>
</ul></li>
<li>There are \(N^2*M\) possible interactions for \(N\) features and \(M\) operations:
<ul>
<li>Reduce them by feature selection (e.g., with RF) or matrix factorization.</li>
</ul></li>
<li>Such approach can be generalized for higher orders.</li>
<li>Due to the fact that number of features grows rapidly with order, they are often constructed semi-manually.</li>
<li>Extract interactions via decision trees by using indices of the leafs (<code>model.apply</code>).
<ul>
<li>Two factors that are split in succession might indicate an interaction.</li>
<li><a href="https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/">Facebook Research's paper about extracting categorical features from trees</a></li>
<li><a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html">Example: Feature transformations with ensembles of trees (sklearn)</a></li>
</ul></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="matrix-factorization"></a><a href="#matrix-factorization" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Matrix factorization</h2>
<ul>
<li>Matrix factorization is a generic approach for dimensionality reduction and feature extraction.</li>
<li>Useful as a dimensionality reduction technique:
<ul>
<li>Dimensionality reduction helps in filtering out not important features.</li>
<li>For example, for higher dimensions, clustering algorithms have a difficult time figuring out which features are the most important, resulting in noisier clusters.</li>
</ul></li>
<li>Can provide additional diversity which is good for ensembles.</li>
<li>It is a lossy transformation which efficiency depends on the task and the number of latent factors.</li>
<li>The same transformation tricks as for linear models should be used.</li>
<li>The same parameters should be used throughout the dataset as it's a trainable transformation.</li>
<li><a href="http://scikit-learn.org/stable/modules/decomposition.html">Overview of Matrix Decomposition methods (sklearn)</a></li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="pca"></a><a href="#pca" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>PCA</h3>
<ul>
<li>PCA (Principal Component Analysis) performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized.
<ul>
<li>Features with low variance contribute less to the separation of objects.</li>
<li>For example, a constant feature means it doesn't have any distinguishing information.</li>
</ul></li>
<li>Works by calculating the eigenvectors from the covariance matrix.</li>
<li>Creates components in the order of causing the most to least variance.
<ul>
<li>In practice, we would create as many components as possible and from them, choose the top \(N\) principal components that can explain 80-90% of the initial data dispersion (via the <code>explained_variance_ratio</code>)</li>
<li>We can then examine the makeup of each PCA component based on the weightings of the original features that are included in that component.</li>
<li><a href="https://aws.amazon.com/blogs/machine-learning/analyze-us-census-data-for-population-segmentation-using-amazon-sagemaker/">Analyze US census data for population segmentation using Amazon SageMaker</a>
<center><img width=400 src="/datadocs/assets/census-sagemaker-2.gif"/></center>
<center><a href="https://aws.amazon.com/blogs/machine-learning/analyze-us-census-data-for-population-segmentation-using-amazon-sagemaker/" class="credit">Credit</a></center></li>
</ul></li>
<li>Helps decorrelating correlated features.
<ul>
<li>Can drop the least important feature while still retaining the most valuable parts.</li>
</ul></li>
<li>Each of the new features or components created after PCA are all independent of one another.</li>
<li>Mainly applied to dense data.</li>
<li>Limitations:
<ul>
<li>PCA is a linear algorithm (not able to interpret complex polynomial relationships between features).</li>
<li>Visualization and interpretation difficulties.</li>
<li>Strongly focused on variance which may not correlate with predictive power.</li>
</ul></li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="svd"></a><a href="#svd" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SVD</h3>
<ul>
<li>SVD (Singular Value Decomposition) is a factorization of a matrix.</li>
<li>Same advantages and limitations as for PCA but faster.</li>
<li><code>TruncatedSVD</code> can be applied to sparse matrices.</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="nmf"></a><a href="#nmf" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>NMF</h3>
<ul>
<li>NMF (Nonnegative Matrix Factorization) is a state of the art feature extraction algorithm.</li>
<li>Automatically extracts sparse and meaningful features from a set of nonnegative data vectors (counts-like data).</li>
<li>NMF decomposes a data matrix \(V\) into the product of two lower rank matrices \(W\) and \(H\) so that \(V\approx{W*H}\).</li>
</ul>
<p><center><img width=550 src="/datadocs/assets/holdout.png"/></center>
<center><a href="http://alexhwilliams.info/itsneuronalblog/2018/02/26/crossval/" class="credit">Credit</a></center></p>
<ul>
<li>Primarily used for recommender systems and text mining.</li>
<li>Provides an additive basis to represent the data.</li>
<li>Results are easier to interpret than SVD.</li>
<li>Transforms data in a way best-suited for tree-based models.</li>
<li>NMF typically benefits from normalization.</li>
<li>Limitations:
<ul>
<li>As opposed to the unconstrained problem which can be solved efficiently using the SVD, NMF is NP-hard.
<ul>
<li>Fortunately there are heuristic approximations.</li>
</ul></li>
<li>There is no guarantee to be a single unique decomposition.</li>
<li>It’s hard to know how to choose the factorisation rank \(r\).
<ul>
<li>Some approaches include trial and error.</li>
</ul></li>
</ul></li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="t-sne"></a><a href="#t-sne" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>t-SNE</h3>
<ul>
<li>t-SNE (t-Distributed Stochastic Neighbor Embedding) is a tool to visualize high-dimensional data.</li>
<li>Unlike PCA, is not a linear projection but allows to capture a non-linear structure.</li>
</ul>
<p><center><img width=250 src="/datadocs/assets/PCASwiss.png"/></center>
<center><a href="https://www.biostars.org/p/295174/" class="credit">Credit</a></center></p>
<ul>
<li>Projects points from high to low-dimensional space by preserving the relative distance between them.
<ul>
<li>Neighbor embedding is a search for a new and less-dimensional data representation that preserves neighborship of examples.</li>
</ul></li>
<li>Optimizes the embeddings directly using gradient descent.</li>
<li>The principal components can be used as features.</li>
<li>Mainly a data exploration and visualization technique.</li>
<li>Apply t-SNE to concatenation of train and test and split projection back.</li>
<li>Limitations:
<ul>
<li>Computationally expensive and can take several hours on million-sample datasets :
<ul>
<li>It is common to do dimensionality reduction before projection.</li>
<li>Use stand-alone implementation <code>tsne</code> for faster speed.</li>
</ul></li>
<li>Sometimes it works well for visualization but not for dimensionality reduction.</li>
<li>Results strongly depend on hyperparameters (perplexity):
<ul>
<li>Good practice is to use several projections with different perplexities (5-100).</li>
<li><a href="http://scikit-learn.org/stable/auto_examples/manifold/plot_t_sne_perplexity.html#sphx-glr-auto-examples-manifold-plot-t-sne-perplexity-py">Example: tSNE with different perplexities (sklearn)</a></li>
</ul></li>
<li>Due to stochastic nature, tSNE may provide different projects for the same configuration.</li>
<li>PCA it is a mathematical technique, but t-SNE is a probabilistic one.</li>
<li>There is the risk of getting stuck in local minima.</li>
</ul></li>
<li><a href="https://github.com/DmitryUlyanov/Multicore-TSNE">Multicore t-SNE implementation</a></li>
<li><a href="http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html">Comparison of Manifold Learning methods (sklearn)</a></li>
<li><a href="https://distill.pub/2016/misread-tsne/">How to Use t-SNE Effectively (distill.pub blog)</a></li>
</ul>
</span></div></article></div><div class="docLastUpdate"><em>Last updated on 2019-9-27 by Oleg Polakow</em></div><div class="docs-prevnext"><a class="docs-prev button" href="/datadocs/docs/machine-learning/feature-engineering"><span class="arrow-prev">← </span><span>Feature Engineering</span></a><a class="docs-next button" href="/datadocs/docs/machine-learning/metric-optimization"><span>Metric Optimization</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#group-based-features">Group-based features</a></li><li><a href="#distance-based-features">Distance-based features</a><ul class="toc-headings"><li><a href="#k-nn">k-NN</a></li><li><a href="#k-means">K-means</a></li></ul></li><li><a href="#bumper-features">Bumper features</a></li><li><a href="#feature-interactions">Feature interactions</a></li><li><a href="#matrix-factorization">Matrix factorization</a><ul class="toc-headings"><li><a href="#pca">PCA</a></li><li><a href="#svd">SVD</a></li><li><a href="#nmf">NMF</a></li><li><a href="#t-sne">t-SNE</a></li></ul></li></ul></nav></div><footer class="nav-footer" id="footer"><div class="brand-box"><div class="brand"><a href="https://www.tum.de/nc/en/" target="_blank" rel="noreferrer noopener" class="brand-link"><img src="/datadocs/img/tum_logo.png" alt="Technical University of Munich" height="45"/></a></div><div class="brand"><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="brand-link"><img src="/datadocs/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a></div></div><section class="copyright">Copyright © 2019 polakowo.io</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '6642fca03d716a543ac4428d7d20b842',
                indexName: 'polakowo-datadocs',
                inputSelector: '#search_input_react',
                algoliaOptions: {}
              });
            </script></body></html>